<!DOCTYPE html>
<head>
<style>
body {
  padding: 100px;
  width: 1000px;
  margin: auto;
  text-align: left;
  font-weight: 250;
  font-family: 'Avenir', sans-serif;
}

div.column {
  float: left;
  width: 50%;
}

div.row:after {
  content: "";
  display: table;
  clear: both;
}

a {
  color: blue;
}

a:hover {
  color: magenta;
}

div.padded {  
  padding-right: 50px;  
  padding-left: 50px;  
}  

pre.indent {
  text-indent: 5%
}

table {
  border-spacing: 40px 0;
}

table.mosaic {
  border-spacing: 0 0;
}

th.img, td.img {
  border-bottom: 1px solid #ddd;
  text-align: center;
  padding: 1rem;
}


th.data, td.data {
  border-bottom: 1px solid #ddd;
  text-align: center;
  padding: 8px;
}

center.mosaic {
  padding: 40px;
}

tr.data:nth-child(even) {background-color: #f2f2f2;}

</style>
<title>CS 194 [Auto]Stitching Photo Mosaics</title>
</head>

<div class="padded">
<a name="top"></a>
<center>
  <h1>Project 5: [Auto]Stitching Photo Mosaics</h1>
  <p>By: Alex Yang</p>
</center>

<div style="float: left; width: 50%;">
  <p><a href="#part1">Part 1: Image Warping and Mosaicing</a></p>
  <ul>
    <li><a href="#part1a">Recover Homographies</a></li>
    <li><a href="#part1b">Warp the Images</a></li>
    <li><a href="#part1c">Image Rectification</a></li>
    <li><a href="#part1d">Making Mosaics</a></li>
    <li><a href="#part1e">What I learned</a></li>
  </ul>
</div>

<div style="float: right; width: 50%;">
  <p><a href="#part2">Part 2: Feature Matching for Autostitching</a></p>
  <ul>
    <li><a href="#part2a">Detecting Corners using Harris Interest Point Detector</a></li>
    <li><a href="#part2b">Extracting Feature Descriptors</a></li>
    <li><a href="#part2c">Feature Matching</a></li>
    <li><a href="#part2d">RANSAC</a></li>
    <li><a href="#part2e">Autostitching Results</a></li>
    <li><a href="#part2f">What I learned</a></li>
  </ul>
</div>

<br></br>
<br></br>
<br></br>
<br></br>
<br></br>
<hr>
<a name="part1"></a>
<h2>Part 1: Image Warping and Mosaicing</h2>
<p>This section covers finding homographies, transforming images, and stitching together transformed images to create mosaics.</p>
<a name="part1a"></a>
<h3>Recover Homographies</h3>
<p>Homographies relate two images of the same planar surface. It is a transformation from one projective space to another, and so it can be written as p' = Hp where H is a 3x3 matrix and p and p' are the projective spaces.</p>

<p>In this part, we recover homographies between two sets of points. We establish p' = Hp for all corresponding points p' and p, and solve for the homography H. Specifically, we want to minimize the loss given by our homography:i</p>
<center>
  <img src="./images/homloss.png" align="middle" width="40%">
</center>
<p>The above can be written as a series of linear equations. The 8x1 vector can be solved using least squares, and to fully recover the homography we add 1 to the end of the vector and reshape it into a 3x3 matrix.</p>
<center>
  <img src="./images/homequations.png" align="middle" width="50%">
</center>
<p>The process of defining correspondence points was done manually by selecting matching locations between images.</p>

<a name="part1b"></a>
<h3>Warp the Images</h3>
<p>We can now warp images using the recovered homography. Because we are applying a perspective transform, the resulting image will have a different shape than the original. So, we first get the bounding box of the warped image by piping the corners of our original image through the homography. The warped corner points define a trapezoidal region which includes our warped image. We now use inverse mapping of the homography to map the points in the trapezoidal region to points in the original image. Each pixel is sampled using linear interpolation. The homography may map points to have negative values, so horizontal and vertical shifting are also needed to adjust for this.</p>

<a name="part1c"></a>
<h3>Image Rectification</h3>
<p>Image rectification is the process of applying a homography to an image usually for straightening, but it can also be more generally used to view images from a different perspective, resulting in some pretty cool effects.</p>
<p>The process of doing this is simple. First define some correspondence points on an image &mdash; four points usually work well. Next, define correpondence points on where the other correspondence points should map to. Find the homography that relate these two sets of points and warp the original image with the homography.</p>
<p>Here is an example I did to make my pencil pouch appear from a bird's-eye view. On the left is the original image taken from a slant, and on the right is the rectified image.</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/pocket.png" align="middle" height="400">
        <figcaption align="middle">Original Image</figcaption>
      </td>
      <td>
        <img src="./images/pocket_rectified.png" align="middle" height="400">
        <figcaption align="middle">Warped Image</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Another example &mdash; Wheeler Hall in Berkeley.</p>
<div align="middle">
  <table style="width=100%" >
    <tr>
      <td>
        <img src="./images/wheeler.png" align="middle" width="100%">
        <figcaption align="middle">Original Image</figcaption>
      </td>
      <td>
        <img src="./images/wheeler_rectified.png" align="middle" width="100%">
        <figcaption align="middle">Warped Image</figcaption>
      </td>
    </tr>
  </table>
</div>

<a name="part1d"></a>
<h3>Making Mosaics</h3>
<p>Let's see how we can use our homographies and warp function to create mosaics. I took two photos for each mosaic, rotating the camera along the z-axis in between photos so that I could capture a wider view. Overlapping features in the photos are necessary so that we can define correspondence points between the two images. Example is shown below:</p>
<div align="middle">
  <table style="width=100%" >
    <tr>
      <td>
        <img src="./images/memorial1.png" align="middle" width="100%">
        <figcaption align="middle">First Shot</figcaption>
      </td>
      <td>
        <img src="./images/memorial2.png" align="middle" width="100%">
        <figcaption align="middle">Second Shot</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Next, I labeled eight correspondence points between the two pictures. I then found the homography H between the points, warped the second image by H, and stitched the original and warped image together.</p>
<p>In order to stitch the images, I first created a blank canvas in which I could stack the images onto. The warped image goes on first, then the original image on top, shifting the images accordingly before stacking. This essentially completes the process, but leaves some edge artifacts around the overlapping region. I used weighted averaging (feathering) to minimize these artifacts.</p>
<p>I first tried linear blending, which involves using a linearly decreasing weight from 1 to 0 over the overlapping region. Specifically, the pixel value in the overlapping region is determined by <font color="teal">rgb = alpha * im1 + (1 - alpha) * im2</font>, where alpha is the weight of the original image's pixel values in the final image for a column in the overlapping region. The weight alpha decreases linearly from 1 to 0 over the columns in the overlapping region for linear blending. This worked well in getting rid of edge artifacts, but also created wedge-like artifacts at the crossing sections. So, instead of a linearly decreasing weight, I tried using a sigmoid function to determine the weight alpha. Now columns closer to the left and right edges of the overlapping region will be influenced even more by the left and right images, respectively. This helped reduce the effect of the wedge artifacts.</p>
<p>Here are the mosaics of Doe Library:</p>
<div align="middle">
  <table style="width=100%" class="mosaic">
    <tr>
      <td>
        <img src="./images/memorial_overlap.png" align="middle" width="100%">
        <figcaption align="middle">Simple Overlap</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="./images/memorial_linear.png" align="middle" width="100%">
        <figcaption align="middle">Linear</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="./images/memorial_sigmoid.png" align="middle" width="100%">
        <figcaption align="middle">Sigmoid</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>More examples:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/bayview1.png" align="middle" width="100%">
        <figcaption align="middle">Bay View Shot 1</figcaption>
      </td>
      <td>
        <img src="./images/bayview2.png" align="middle" width="100%">
        <figcaption align="middle">Bay View Shot 2</figcaption>
      </td>
    </tr>
  </table>
</div>
<center class="mosaic">
  <img src="./images/bayview_sigmoid.png" align="middle" width="100%">
  <figcaption align="middle">Bay View Mosaic (Sigmoid)</figcaption>
</center>
<br></br>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/campanile1.png" align="middle" width="100%">
        <figcaption align="middle">Campanile Shot 1</figcaption>
      </td>
      <td>
        <img src="./images/campanile2.png" align="middle" width="100%">
        <figcaption align="middle">Campanile Shot 2</figcaption>
      </td>
    </tr>
  </table>
</div>
<center class="mosaic">
  <img src="./images/campanile_sigmoid.png" align="middle" width="100%">
  <figcaption align="middle">Campanile Mosaic (Sigmoid)</figcaption>
</center>
<br></br>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/uppersproul1.png" align="middle" width="100%">
        <figcaption align="middle">Upper Sproul Shot 1</figcaption>
      </td>
      <td>
        <img src="./images/uppersproul2.png" align="middle" width="100%">
        <figcaption align="middle">Upper Sproul Shot 2</figcaption>
      </td>
    </tr>
  </table>
</div>
<center class="mosaic">
  <img src="./images/uppersproul_sigmoid.png" align="middle" width="100%">
  <figcaption align="middle">Upper Sproul Mosaic (Sigmoid)</figcaption>
</center>

<a name="part1e"></a>
<h3>What I learned</h3>
<p>It was really cool learning how a simple homography can do so much. I think the best part was image rectification because the images really look like they were taken from another perspective even though I just multiplied the image by a homography matrix.</p>

<p><a href="#top">[Back to top]</a></p>

<hr>
<br></br>

<a name="part2"></a>
<h2>Part 2: Feature Matching for Autostitching</h2>
<p>This section covers the process of automatically making mosaics. It heavily draws from <a href=https://inst.eecs.berkeley.edu/~cs194-26/sp20/Papers/MOPS.pdf">"Multi-Image Matching using Multi-Scale Oriented Patches" by Brown et al.</a></p>

<a name="part2a"></a>
<h3>Detecting Corners using Harris Interest Point Detector</h3>
<p>In order to find features to use as our correspondence points, we must first detect interest points in the images we want to stitch. We use Harris Interest Point Detector to get all potential interest points. However, we discard any interest points that are within 20 pixels of the image border, for reasons that will be seen in the next section (Extracting Feature Descriptors).</p>
<p>Using the same photos taken of Doe Library and the Campanile, we run the detector and get the following interest points in red:</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/memorial1_harris.png" align="middle" width="100%">
        <figcaption align="middle">Memorial Shot 1 - Harris Points</figcaption>
      </td>
      <td>
        <img src="./images/memorial2_harris.png" align="middle" width="100%">
        <figcaption align="middle">Memorial Shot 2 - Harris Points</figcaption>
      </td>
    </tr>
  </table>
</div>

<h4>Adaptive Non-Maximal Suppression (ANMS)</h4>
<p>Harris Interest Point Detector gives a great start, but it's time to start filtering out interest points that aren't that interesting. One method is to just take the interest points with the highest Harris values. However, this results in interest points that are heavily clustered together around clearly defined corners. Having interest points that are spacially distributed is preferred because it will help our homography generalize well across the entire image, rather than just a few areas.</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/memorial1_highest_harris.png" align="middle" width="100%">
        <figcaption align="middle">Interest points with highest Harris values (n=500)</figcaption>
      </td>
      <td>
        <img src="./images/memorial2_highest_harris.png" align="middle" width="100%">
        <figcaption align="middle">Interest points with highest Harris values (n=500)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>In order to get more evenly distributed interest points, we'll use adaptive non-maximal suppression (ANMS) as outlined in Brown et al. ANMS only keeps interest points that are the strongest in it's neighborhood of points. Strength is still determined by Harris value, but ANMS will only choose a few points in a neighborhood so we don't get as much clustering.</p>
<p>In ANMS, each interst point is ranked in decreasing order by it's distance to the closest interest point that is significantly stronger. In other words, a point's suppression radius is determined by the minimum distance to another interest point that has Harris response at least 1/c * h, where c is a robustness parameter in [0,1) and h is the Harris response of the first point. We do this for all points and sort by suppression radius in decreasing order. We then take the top 500 points, for example. This process can be vectorized for efficiency.</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/memorial1_harris_anms.png" align="middle" width="100%">
        <figcaption align="middle">Interest points after ANMS (n=500)</figcaption>
      </td>
      <td>
        <img src="./images/memorial2_harris_anms.png" align="middle" width="100%">
        <figcaption align="middle">Interest points after ANMS (n=500)</figcaption>
      </td>
    </tr>
  </table>
</div>

<a name="part2b"></a>
<h3>Extracting Feature Descriptors</h3>
<p>We now extract feature descriptors in preparation for Feature Matching (next section). Each feature descriptor is an axis-aligned 8x8 patch sampled from a larger 40x40 window around an interest point. Basically, we blur the 40x40 window around an interest point with a Gaussian filter, and then downsample to an 8x8 patch. Blurring and downsampling help our features be more robust. We also standardize the descriptor vector (mean=0, std=1) so that the features are invariant to affine changes in intensity.</p>
<p>Here are the descriptors for corresponding points. The patch is from the upper right corner of one of the library windows.</p>
<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="./images/mem1_feature.png" align="middle" width="100%" border="1">
        <figcaption align="middle">Window patch in shot 1</figcaption>
      </td>
      <td>
        <img src="./images/mem2_feature.png" align="middle" width="100%" border="1">
        <figcaption align="middle">Window patch in shot 2</figcaption>
      </td>
    </tr>
  </table>
</div>

<a name="part2c"></a>
<h3>Feature Matching</h3>
<p>To match features across images, we use SSD to find the distance between all feature descriptors in one image and all descriptors in the other image. We then use Lowe thresholding to create a match. Lowe thresholding uses the ratio of the errors of the first nearest neighbor and second nearest neighbor. A feature descriptor is only matched to it's nearest neighbor in the other image and if the ratio of the error of the nearest neighbor to the error of the second nearest neighbor is below a threshold. We set this threshold as 0.5, as suggested in Brown et al. We also enforce that the two matched descriptors must be each other's nearest neighbor.</p>
<p>Here are the matched features, labeled. You can see that the process does well for the most part, but there are some incorrect matches.</p>
<div align="middle">
  <table style="width=100%" class="mosaic">
    <tr>
      <td>
        <img src="./images/memorial1_desc_points.png" align="middle" width="100%">
        <figcaption align="middle">Memorial shot 1 - 50 matched features</figcaption>
      </td>
      <td>
        <img src="./images/memorial2_desc_points.png" align="middle" width="100%">
        <figcaption align="middle">Memorial shot 2 - 50 matched features</figcaption>
      </td>
    </tr>
  </table>
</div>

<a name="part2d"></a>
<h3>Random Sample Consensus (RANSAC)</h3>
<p>We now use Random Sample Consensus (RANSAC) to robustify our feature matching. Because of the sensitivity to outliers in least squares, incorrect matches (outliers) left from the previous part will throw off our homography calculation if kept in the set of correspondence points. We deploy RANSAC into our autostitching process to remove incorrect feature matches.</p>
<p>RANSAC is an iterative process. In each iteration, we sample four different pairs of matching points, and compute the homography H between them. We then transform every interest point in the first image by H. We now retrieve the set of inliers, which are points that are within one pixel away from their correspondence after being transformed. The set of points returned by RANSAC is simply the largest set of inliers.</p>
<p>Here are the matched features after running RANSAC for 10,000 iterations. These are also the correspondence points we'll use to stitch the two images.</p>
<div align="middle">
  <table style="width=100%" class="mosaic">
    <tr>
      <td>
        <img src="./images/memorial1_points.png" align="middle" width="100%">
        <figcaption align="middle">Memorial shot 1 - 21 correspondence points</figcaption>
      </td>
      <td>
        <img src="./images/memorial2_points.png" align="middle" width="100%">
        <figcaption align="middle">Memorial shot 2 - 21 correspondence points</figcaption>
      </td>
    </tr>
  </table>
</div>

<a name="part2e"></a>
<h3>Autostitching Results</h3>
<p>This is the resulting image of Doe library after autostitching.</p>
<center>
  <img src="./images/memorial_autostitch.png" align="middle" width="100%">
</center>
<br></br>
<p>Here are some comparisons. On the top are mosaics stitched using manually labeled correspondences, and on the bottom are autostitched images. Autostitching really helps unblur the mosaics, probably because my manual correspondence points weren't very precise.</p>
<center>
  <figcaption align="middle">Memorial Glade</figcaption>
  <img src="./images/memorial_sigmoid.png" align="middle" width="100%">
  <img src="./images/memorial_autostitch.png" align="middle" width="100%">
</center>
<br></br>
<center>
  <figcaption align="middle">Bay View</figcaption>
  <img src="./images/bayview_sigmoid.png" align="middle" width="100%">
  <img src="./images/bayview_autostitch.png" align="middle" width="100%">
</center>
<br></br>
<center>
  <figcaption align="middle">Campanile</figcaption>
  <img src="./images/campanile_sigmoid.png" align="middle" width="100%">
  <img src="./images/campanile_autostitch.png" align="middle" width="100%">
</center>
<br></br>
<center>
  <figcaption align="middle">Upper Sproul</figcaption>
  <img src="./images/uppersproul_sigmoid.png" align="middle" width="100%">
  <img src="./images/uppersproul_autostitch.png" align="middle" width="100%">
</center>
<br></br>
<p>And some fun images:</p>
<center>
  <img src="./images/couch_autostitch.png" align="middle" width="100%">
  <img src="./images/fingerguns_autostitch.png" align="middle" width="100%">
</center>

<a name="part2f"></a>
<h3>What I learned</h3>
<p>I think RANSAC was the coolest thing about this part. I like the idea that it seperates the structure from the noise. Even if there's much more noise, all the noise is different so the structural elements can still be found by RANSAC.</p>

<p><a href="#top">[Back to top]</a></p>

</div>

